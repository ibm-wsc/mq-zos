{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IBM MQ for z/OS Enablement for the IBM Washington Systems Center","text":""},{"location":"#at-a-glance","title":"At a glance","text":"<p>This site is a repository for IBM WSC intellectual capital related to IBM MQ for z/OS. It includes labs, presentations, and document materials. To request specific enablement, demonstrations or labs, make a request to your IBM representative to engage the Washington Systems Center.</p>"},{"location":"#other-ibm-mq-resources-not-specific-to-zos","title":"Other IBM MQ resources - not specific to z/OS","text":"<p>IBM Developer - MQ Materials</p> <p>IBM Integration Community Blog</p> <p>IBM Messaging GitHub</p> <p>IBM MQ Documentation</p> <p>IBM MQ Performance</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>Contributors to the site include Lyn Elkins, Mitch Johnson, and Dorothy Quincy</p>"},{"location":"archive/","title":"Archive","text":"<p>Assets listed here may be outdated or no longer offered, but we have included them as reference.</p> Asset Author Intro to MQ SMF - Buffer Pool Stats Lyn Elkins Intro to MQ SMF - Buffer Pool Stats pt. 2 Lyn Elkins Intro to MQ SMF Lyn Elkins Intro to MQ SMF - Data gathering Lyn Elkins"},{"location":"archive/#ibm-share-2022-lab-exercises","title":"IBM SHARE 2022 Lab Exercises","text":"Asset Ansible on IBM MQ lab Queue Statistics lab Streaming queues on IBM MQ for multiplatforms Streaming queues on IBM MQ for z/OS"},{"location":"comparing-offloading/","title":"Lab Exercise: Comparing SMDS and DB2 Blobs for queue-sharing","text":""},{"location":"comparing-offloading/#audience-level-knowledge-of-mq-or-zos","title":"Audience level: knowledge of MQ or z/OS","text":""},{"location":"comparing-offloading/#skillset-zos-systems-programming-mq-administration","title":"Skillset: z/OS Systems Programming, MQ Administration","text":""},{"location":"comparing-offloading/#background","title":"Background","text":"<p>Shared message data sets (SMDS) are the preferred method for offloading large messages in queue-sharing groups. SMDS\u2019s are designed to handle large messages efficiently, so in this exercise, we will test two CF structures, one with SMDS and the other with BLOBs to examine the differences between the two offloading mechanisms.</p>"},{"location":"comparing-offloading/#overview-of-exercise","title":"Overview of exercise","text":"<p>I.  Run OEMPUT program against SMDS-enabled CF structure (TEST1)</p> <p>II. Run OEMPUT program against BLOB-enabled CF structure (TEST2)</p> <p>III.    Compare the output from both</p>"},{"location":"comparing-offloading/#steps-of-exercise","title":"Steps of exercise","text":""},{"location":"comparing-offloading/#i-run-oemput-program-against-smds-enabled-cf-structure-test1","title":"I. Run OEMPUT program against SMDS-enabled CF structure (TEST1)","text":"<ol> <li> <p>Using MQ Explorer, verify that the below configuration is in place. You should see connections to ZQS1, ZQS2, and you should see a QSGA queue-sharing group visible.</p> </li> <li> <p>In MQ Explorer, navigate to the queue-sharing group QSGA\u2019s Coupling Facility Structures by clicking  \u2018&gt;\u2019 next to the QSGA label then pressing Coupling Facility Structures to display more information.</p> </li> <li> <p>Your structures should look like the following:     </p> </li> <li> <p>Scroll to the right, making sure that all offload rules are the same for TEST1 and TEST2 except for the \u2018Offload\u2019 and \u2018Group data set name\u2019 fields.</p> <p></p> </li> <li> <p>Now, navigate to the MQS1 z/OS image.</p> </li> <li> <p>Use option 3.4 to navigate to the ZQS1.MQ.JCL data set. Navigate to PUTSMDS and type an \u2018e\u2019 to edit the member.</p> <p></p> </li> <li> <p>In PUTSMDS, you will see an execution of OEMPUT. This JCL puts a large amount of large messages on our SMDS.QUEUE, defined to TEST1. Which parameters are we using with OEMPUT here?</p> </li> </ol> Parameter Description - mZQS1 Specify target queue manager -qSMDS.QUEUE Specify target queue -fileDD:MSGIN Specify messages to be used -ts500 Specify how long message stream should last (500 seconds) -s650000 Specify the size of the message, note we are using a message larger than 63KB here, as to necessitate the use of offloading, since large messages can\u2019t be held in the CF list structures. -l10 Loop MQPUT and MQGET 10 times during execution -cgcpc Mimic client application program by procressing a commit after both MQPUTs and MQGETs -crlf Each line in the input message file is used in sequence as message data -rSMDS.QUEUE Reply-to-queue from which replies will be retrieved (MQGET). If the -r option is omitted, MQGETs will not be issued. <p>**Note: If we specified persistent messages here, the contrast between SMDS and BLOBs would be less noticeable because transactions on both sides would have to wait on logging.</p> <ol> <li>Type \u2018submit\u2019 in the command line and press your enter key. You should see a reason code (RC) of 0000. This execution will take a few minutes to complete.</li> </ol>"},{"location":"comparing-offloading/#ii-run-oemput-program-against-blob-enabled-cf-structure-test2","title":"II. Run OEMPUT program against BLOB-enabled CF structure (TEST2)","text":"<ol> <li> <p>Now, we will repeat the steps to submit another execution of OEMPUT, this time for our queue tied to BLOB storage. </p> </li> <li> <p>Use F3 to back out to the ZQS1.MQ.JCL data set. Place an \u2018E\u2019 next to PUTBLOB and press enter to edit the member. </p> </li> <li> <p>As you look through PUTBLOB, navigating up and down the screen using the F7 and F8 keys, you will notice that the only difference between PUTSMDS and PUTBLOB is the queue name. We are keep all other variables constant, especially message size.</p> </li> <li> <p>Type \u2018submit\u2019 in the command line and press your enter key. You should see a reason code (RC) of 0000. This execution will take a few minutes to complete. </p> </li> </ol>"},{"location":"comparing-offloading/#iii-compare-the-output-from-both","title":"III. Compare the output from both","text":"<ol> <li> <p>From the ISPF main menu, type \u2018sdsf\u2019 or \u2018d\u2019 on the command line and press enter to access the SDSF menu. </p> </li> <li> <p>Here, type \u2018ST\u2019 on the command line and press enter to access the status of recent jobs.</p> </li> <li> <p>In our JCL, our OEMPUT jobs were named OEMPSMDS and OEMBLOB, respectively. We can search for all jobs beginning with OEM, but typing in the command line \u2018pre OEM*\u2019 and pressing enter.</p> </li> <li> <p>Both jobs should appear in a list. Let\u2019s look at OEMPSMDS first. Place a question mark to the left of the job name and press enter. </p> </li> <li> <p>A list should appear with output on how successful the job was and any output from the job. We are interested in the SYSPRINT output. Place a \u2018s\u2019 to the left of SYSPRINT and press enter.</p> </li> <li> <p>Scroll down on the SYSPRINT output until you see the following output. Make not of the Total Transaction value, the Transaction Rate value, and the Avg App CPU per msg value. This gives us information about how many transactions were completed in the allotted time with SMDS storage specified, the efficiency of those transactions, and the CPU consumption required.</p> </li> </ol> <p>If you are unable to see the SYSPRINT screen for any reason, we have prepared sample examples at the end of this lab for reference.</p> <ol> <li> <p>Now, let\u2019s check out the same information for BLOB storage. Use F3 to back out twice until you reach the list containing OEMPSMDS and OEMPBLOB.</p> </li> <li> <p>Place a \u2018?\u2019 next to the OEMPBLOB job and press enter.</p> </li> <li> <p>Place a \u2018s\u2019 next to the SYSPRINT output and press enter.</p> </li> <li> <p>Navigate until you see the Total Transaction value, the Transaction Rate value, and the Avg App CPU per msg value.</p> </li> </ol> <p>If you are unable to see the SYSPRINT screen for any reason, we have prepared sample examples at the end of this lab for reference.</p> <ol> <li>You have now compared the performance and storage consumption of SMDS and BLOB offloading in our test environment! Hopefully, this helps you see the advantages of using SMDS in terms of throughput. While CPU consumption is higher for SMDS in this test environment, </li> </ol> <p>Figure 1. SMDS performance</p> <p></p> <p>Figure 2 BLOB performance</p> <p></p>"},{"location":"connectivity/","title":"Connecting to your z/OS Sysplex MQS1","text":""},{"location":"connectivity/#audience-level","title":"Audience level","text":"<p>Some knowledge of MQ or z/OS </p>"},{"location":"connectivity/#skillset","title":"Skillset","text":"<p>z/OS Systems Programming, MQ Administration \u2003</p>"},{"location":"connectivity/#introduction","title":"Introduction","text":"<p>This lab is designed to connect the user to the Sysplex they have been assigned.  It will cover a couple of different connections (PCOMM, MQ Explorer) and some basic queue manager commands. </p>"},{"location":"connectivity/#lab-steps","title":"Lab Steps","text":"<p>1)  If you have not already done so, please connect to the browser instance you were assigned.  Note that your user ID is always Administrator and the password is associated with the unique URL.  </p> <p>2)  Please look on the panel for a PCOMM connection symbol (  ) labeled \u2018MQS1\u2019 or \u2018MQS2.\u2019   If you do not see it, please follow these steps to get it added: a.  From the Windows programs, please expand the IBM Personal Communications group and select \u2018Start or Configure Sessions\u2019 as shown here:</p> <p></p> <p>b.  Some of our Windows images do not have the correct directory for PCOMM sessions.  If the Session Manager pane comes up empty, please change the directory that PCOMM uses. </p> <pre><code>i.  In the upper left had corner of the Session manager pane, please chose \u2018File\u2019 then Change Directory\n\n    ![Picture of session manager pane](assets/conn2.png)\n\nii. Select the Documents directory\n\n    ![Picture of Documents window](assets/conn3.png)\n\niii.    If you see the MQS1 \u2018WS\u2019 file, please click on \u2018Open\u2019 as shown.\n\n    ![Picture of Documents window](assets/conn4.png)\n\niv. IF you do not see the \u2018mqs1.WS\u2019 file, please notify Dorothy or Lyn immediately.  We may need to reset your image.\n\nv.  If you do see the file, after clicking on Open, your \u2018Session Manager\u2019 pane should look something like this:\n</code></pre> <p>c.  Highlight \u2018mqs1\u2019 and click on the \u2018Start\u2019 button. 3)  If you do see the \u2018mqs1\u2019 icon, please click on it. 4)  At this point the WSC MQPLEX1 should be shown.  It may take a minute or so to connect. 5)  Enter \u2018mqs1 user1\u2019 and press the enter key (note it may be the right hand \u2018ctrl\u2019 key or the actual enter key depending on your keyboard map.  If you would like to change the enter key and do not know how to alter your keyboard, please ask Dorothy or Lyn for assistance. </p> <p>6)  At this point you should see the TSO logon screen.  Please use the enter key to take you to the ISPF manu.</p> <p>7)  The menu looks as follows:</p> <p>8)  You have now successfully connected to the Sysplex LPAR MQS1!  </p> <p>9)  Check to see if the queue manager is running: a.  On the \u2018Option\u2019 line, please enter 13 to navigate to the z/OS User panel. b.  Then enter \u201814\u2019 to select SDSF</p> <p>c.  From the SDSF menu, please select DA and as a convenience change the \u2018scroll\u2019 value from Page to CSR (cursor position) as shown.</p> <p>d.  Enter the command \u2018f ZSQ1\u2019 to search for the queue manager as shown:</p> <p>e.  If the response is  , then the queue manager is not started yet. f.  To start the queue manager, enter the start command as shown:</p> <p>Note that the command prefix for our queue managers is the queue manager name only, we don\u2019t use special characters for these.  g.  You should bet a \u2018NO RESPONSE RECEIVED\u2019 replay, then do a search for ZQS1 again.</p> <p>h.  The started queue manager should then show up in the \u2018Active Users\u2019 list, as seen here: </p> <p>i.  Next, start the channel initiator address space by entering   \\ The response should be: </p> <p>j.  Using the enter key, the CHIN should now show up in the active users list:</p> <p>10) Now that the queue manager and channel initiator have started, it is time to connect the MQ Explorer to the queue manager.  Return to the windows image, Click on the  MQ Explorer as shown here: </p> <p>11) The MQ Explorer panel should look something like this:</p> <p>12) Notice that there is no entry for ZQS1, so we need to add a connection to that queue manager.  To do this, right click on the Queue Managers folder and select \u2018Add remote queue manager.\u2019 13) The \u2018Add queue manager\u2019 pane should appear.  Please fill in the queue manager name ZQS1 \u2013 CASE MATTERS! Then please click on the Next button.</p> <p>14) On the Specify new connection details panel, enter the host name as clone1mqs1 and the port number as 1424 as shown.  Please then click on the \u2018Finish\u2019 button.  </p> <p>The name clone1mqs1 has been created in the etc hosts file for convenience.  </p> <p>IF you have trouble connecting, please ask Dorothy and Lyn for assistance.  </p> <p>15) The queue managers panel is displayed showing the new queue manager, and a new folder for the Queue Sharing Group.  Expanding that folder will show the queue sharing group resources.  </p> <p>Congratulations!  Your Windows image is now fully connected to the back end Sysplex. </p>"},{"location":"mp1b/","title":"Looking at SMF data for problem determination","text":""},{"location":"mp1b/#audience-level","title":"Audience level","text":"<p>Some knowledge of MQ or z/OS </p>"},{"location":"mp1b/#skillset","title":"Skillset","text":"<p>MQ Administration, z/OS systems programming</p>"},{"location":"mp1b/#background","title":"Background","text":"<p>MP1B is a utility provided by IBM to analyze your IBM MQ environment\u2019s performance. MP1B shows you your SMF performance data and allows you to roll it off platform to CSV files for further analysis.</p> <p>MP1B is installable at Link</p> <p>Out of the box, it contains:</p> <p>MQCMD \u2013 a program to display queue statistics and channel status over time</p> <p>MQSMF \u2013 a program for interpreting your own accounting and statistics data</p> <p>OEMPUT - a program to put/get messages in high quantities, useful for testing throughput</p>"},{"location":"mp1b/#overview-of-exercise","title":"Overview of exercise","text":"<p>I.  Set up the local queue MP1B.TESTER</p> <p>II. Make sure settings are in place to record SMF data </p> <p>III.    Run JCL to record our SMF data </p> <p>IV. Navigate the SMF data output to find performance problems in our queue</p> <p>V.  Interpret the performance problem</p> <p>Video tutorial of the following exercise: Link</p>"},{"location":"mp1b/#exercise","title":"Exercise","text":"<ol> <li> <p>MP1B has been installed on this environment, and you can find it by searching for the directory ZQS1.MP1B.JCL in the =3.4 data set search bar.</p> <p></p> </li> <li> <p>Now, outside of z/OS, open up MQ Explorer on your Windows Desktop. The icon should look like this:</p> <p></p> </li> <li> <p>Once you\u2019ve opened MQ Explorer, you should see a left-hand menu bar like below. Right click on the ZQS1 queue manager and hit \u2018Connect\u2019.</p> <p></p> </li> <li> <p>By clicking on the arrow to the left of ZQS1, a dropdown list of MQ objects will appear. Right click on the \u2018Queues\u2019 folder and construct a new local queue called MP1B.TESTER.</p> <p></p> </li> <li> <p>Create a queue on your queue manager using MQ Explorer. The queue should have the following properties: </p> <p></p> </li> </ol> <p>Why make the queue shareable? Great question! Shareable queues tend to come in handy in a test environment, so that developers can browse the queues.</p> <ol> <li> <p>Now that we have our queue defined, head back to z/OS. </p> </li> <li> <p>Now, we will enter a series of MVS commands to adjust the settings of the queue manager to prepare it for the collection of SMF data. To do this, navigate to the ISPF main menu</p> </li> <li> <p>Once in the ISPF main menu, enter \u2018d\u2019 in the command line and hit enter</p> </li> <li> <p>Once in SDSF, place a / in the command input line and hit enter</p> </li> <li> <p>A MVS command prompt like this should pop up:</p> <p></p> </li> <li> <p>Enter the following commands here, one at a time. Each command will take you out of the System Command Extension window, so you will have to use the / command to return to the correct window for executing commands.</p> <pre><code>ZQS1 SET SYSTEM STATIME(1.00)\n</code></pre> <p>To change the statistics time interval to 1 minute</p> <pre><code>ZQS1 SET SYSTEM ACCTIME(-1)\n</code></pre> <p>To change the accounting time interval to match the statistics time interval</p> <pre><code>ZQS1 SET SYSTEM LOGLOAD(200)\n</code></pre> <p>To change the log load attribute to the minimum.</p> <p>We want to modify our queue manager\u2019s log load attribute to be super low in order to manufacture a lot of checkpointing so we see something interesting in the SMF records for the purpose of the lab</p> <pre><code>DISPLAY SMF\n</code></pre> <p>This tells us where our SMF data will be stored</p> <pre><code>ZQS1 ALTER QMGR STATCHL(MEDIUM)\n</code></pre> <p>This tells z/OS we want to enable channel statistics to be collected at a moderate ratio of data collection</p> <pre><code>ZQS1 ALTER QMGR MONQ(MEDIUM)\n</code></pre> <p>This tells z/OS to turn on monitoring for the queue manager\u2019s queues at a moderate ratio of data collection</p> <pre><code>ZQS1 ALTER QMGR MONCHL(MEDIUM)\n</code></pre> <p>This tells z/OS to turn on monitoring for the queue manager\u2019s channels at a moderate ratio of data collection</p> <pre><code>ZQS1 START TRACE(STAT) CLASS(1,2,4,5)\n\nZQS1 START TRACE(ACCTG) CLASS(3,4)\n</code></pre> </li> <li> <p>Now all the settings should be in place for our queue manager. Head back to ZQS1.MP1B.JCL using 3.4 from the main ISPF menu. </p> </li> <li> <p>We will use OEMPUT to load messages into MP1B.TESTER. In the directory ZQS1.MP1B.JCL, place an \u2018e\u2019 to the left of the OEMPUT member. </p> <p></p> </li> <li> <p>Make sure that your queue manager and queue names are correct in lines 46 and 47.</p> </li> <li> <p>Once in OEMPUT, type \u2018submit\u2019 on the command line and hit enter to load persistent messages into the queue manager.</p> <p>I won\u2019t summarize the whole JCL, but pay attention to this particular line:  </p> <p><code>PARM=('-M&amp;QM -tm3 -Q&amp;Q -crlf -fileDD:MSGIN -P')</code></p> <p>Lets break it down:</p> Parameter Meaning '-M&amp;QM Queue manager name -tm3 Send messages for 3 minutes -Q&amp;Q The queue name -crlf Each line in the input message file is used in sequence as message data -fileDD:MSGIN Use the MSGIN file as input -P Use persistent messages </li> <li> <p>If you look at your MQ Explorer, you should now see that your queue is populated with lots of messages! </p> <p></p> </li> <li> <p>Back in ZQS1.MP1B.JCL, navigate to the SMFDUMP member. Once inside, enter \u2018submit\u2019 on the command line to execute SMFDUMP JCL. The SMFDUMP JCL starts with deleting old tasks, then outputs it in a specified location, in our case, ZQS1.QUEUE.MQSMF.SHRSTRM2.</p> <p></p> </li> <li> <p>You can check that the SMFDUMP is processing by navigating to your job using SDSF. Access SDSF using =D from the ISPF menu.</p> </li> <li>Once in SDSF, select ST from the menu and hit \u2018enter\u2019</li> <li>Type in \u2018prefix ZQS1*\u2019. This will show you a list of all jobs submitted that start with ZQS1. Remember, we define our job names at the top left of each JCL file.  </li> <li>Here, you put a \u2018?\u2019 mark besides the jobname. Hit enter, then a screen with a SYSPRINT menu option should pop up. Next to SYSPRINT, put a \u2018s\u2019 and hit enter.</li> <li> <p>Enter \u2018bottom\u2019 on the command line and you should see a screen like below, indicating that records are being written. You can also confirm this by looking in the output for the SUMMARY ACTIVITY REPORT.</p> <p></p> </li> <li> <p>After submitting, you will have to submit another job MQSMFP in ZQS1.MP1B.JCL. This job will give us some formatted information about the SMF data. Make one change before submitting: ensure that the julian date is correct. For labs taking place on 2/24/2025, the julian date is 25055.</p> </li> <li> <p>Type \u2018submit\u2019 and hit enter.</p> <p></p> </li> <li> <p>Now, navigate to the SDSF output for the submitted job. We will be able to see the SMF output in useful categories that can also be exported as CSV files.</p> <p></p> </li> <li> <p>Navigate to the LOG statistics by putting a \u2018s\u2019 next to it and hitting enter. Scroll down until you see a screen similar to the one below. </p> </li> <li> <p>Here you can see LLCheckpoints has a value of 1564. Within our interval, we would expect this value to be 0\u2019s or single-digits. 1564 is way too high. This indicates we should adjust our LOGLOAD attribute to have it write more log records between checkpoints.</p> <p></p> </li> </ol>"},{"location":"mp1b/#summary","title":"Summary","text":"<p>The LOGLOAD parameter specifies the number of log records that are written between checkpoints. In the figure above, you can see the LOGLOAD indicated by the blue brackets. For the above image\u2019s example, the LOGLOAD looks to be 6 here (6 would be impossibly small in a real environment). We set our queue manager\u2019s LOGLOAD attribute to the lowest possible value of 200 then flood our environment with messages. We saw see this cause high checkpointing in our recorded SMF window, resulting in unnecessary consumption of processor time and additional I/O.</p>"},{"location":"queue-statistics/","title":"Evaluating queue performance with queue statistics","text":""},{"location":"queue-statistics/#audience-level","title":"Audience level","text":"<p>Some knowledge of MQ or z/OS </p>"},{"location":"queue-statistics/#skillset","title":"Skillset","text":"<p>MQ Administration, z/OS systems programming</p>"},{"location":"queue-statistics/#background","title":"Background","text":"<p>Queue statistics was introduced into the IBM MQ for z/OS product in continuous delivery version 9.3.3. This lab is a modification of the MP1B performance lab to demonstrate how to access per-queue statistics data. Queue statistics fields are well-documented at the following link: Link</p> <p>For this lab, we will be using MP1B to produce a performance report. MP1B is a utility provided by IBM to analyze your IBM MQ environment\u2019s performance. MP1B shows you your SMF performance data and allows you to roll it off platform to CSV files for further analysis.</p> <p>MP1B is installable at Link</p> <p>Out of the box, it contains:</p> <p>MQCMD \u2013 a program to display queue statistics and channel status over time</p> <p>MQSMF \u2013 a program for interpreting your own statistics data, including queue statistics</p> <p>OEMPUT - a program to put/get messages in high quantities, useful for testing throughput</p>"},{"location":"queue-statistics/#overview-of-exercise","title":"Overview of exercise","text":"<p>I.  Set up the local queue MP1B.TESTER</p> <p>II. Make sure settings are in place to record SMF data </p> <p>III.    Run JCL to record our SMF data </p> <p>IV. Navigate the SMF data output to find performance problems </p> <p>V.  Interpret the performance problem</p>"},{"location":"queue-statistics/#exercise","title":"Exercise","text":""},{"location":"queue-statistics/#i-set-up-the-local-queue-mp1btester","title":"I. Set up the local queue MP1B.TESTER","text":"<ol> <li> <p>MP1B has been installed on this environment, and you can find it by searching for the directory ZQS1.MP1B.JCL in the =3.4 data set search bar.</p> <p></p> </li> <li> <p>Now, outside of z/OS, open up MQ Explorer on your Windows Desktop. The icon should look like this:</p> <p></p> </li> <li> <p>Once you\u2019ve opened MQ Explorer, you should see a left-hand menu bar like below. Right click on the ZQS1 queue manager and hit \u2018Connect\u2019.</p> <p></p> </li> <li> <p>By clicking on the arrow to the left of ZQS1, a dropdown list of MQ objects will appear. Right click on the \u2018Queues\u2019 folder and construct a new local queue called MP1B.TESTER.</p> <p></p> </li> <li> <p>Create a queue on your queue manager using MQ Explorer. The queue should have the following properties: </p> <p></p> </li> </ol> <p>Why make the queue shareable? Great question! Shareable queues tend to come in handy in a test environment, so that developers can browse the queues.</p> <ol> <li>Now that we have our queue defined, head back to z/OS. </li> </ol>"},{"location":"queue-statistics/#ii-make-sure-settings-are-in-place-to-record-smf-data","title":"II.    Make sure settings are in place to record SMF data","text":"<ol> <li> <p>Now, we will enter a series of MVS commands to adjust the settings of the queue manager to prepare it for the collection of SMF data. To do this, navigate to the ISPF main menu</p> </li> <li> <p>Once in the ISPF main menu, enter \u2018d\u2019 in the command line and hit enter</p> </li> <li> <p>Once in SDSF, place a / in the command input line and hit enter</p> </li> <li> <p>A MVS command prompt like this should pop up:</p> <p></p> </li> <li> <p>Enter the following commands here, one at a time. Each command will take you out of the System Command Extension window, so you will have to use the / command to return to the correct window for executing commands.</p> <pre><code>ZQS1 SET SYSTEM STATIME(1.00)\n</code></pre> <p>To change the statistics time interval to 1 minute</p> <p>We want to modify our queue manager\u2019s log load attribute to be super low in order to manufacture a lot of checkpointing so we see something interesting in the SMF records for the purpose of the lab</p> <pre><code>DISPLAY SMF\n</code></pre> <p>This tells us where our SMF data will be stored</p> <pre><code>ZQS1 ALTER QMGR STATCHL(MEDIUM)\n</code></pre> <p>This tells z/OS we want to enable channel statistics to be collected at a moderate ratio of data collection</p> <pre><code>ZQS1 ALTER QMGR MONQ(MEDIUM)\n</code></pre> <p>This tells z/OS to turn on monitoring for the queue manager\u2019s queues at a moderate ratio of data collection</p> <pre><code>ZQS1 ALTER QMGR MONCHL(MEDIUM)\n</code></pre> <p>This tells z/OS to turn on monitoring for the queue manager\u2019s channels at a moderate ratio of data collection</p> <pre><code>ZQS1 START TRACE(STAT) CLASS(1,2,4,5)\n</code></pre> </li> <li> <p>Now all the settings should be in place for our queue manager. Head back to ZQS1.MP1B.JCL using 3.4 from the main ISPF menu. </p> </li> </ol>"},{"location":"queue-statistics/#iii-run-jcl-to-record-our-smf-data","title":"III.    Run JCL to record our SMF data","text":"<ol> <li> <p>We will use OEMPUT to load messages into MP1B.TESTER. In the directory ZQS1.MP1B.JCL, place an \u2018e\u2019 to the left of the OEMPUT member. </p> <pre><code>    //************************************************\n//*                                               \n//  SET QM=ZQS1                                   \n//  SET Q=TEAM1.STREAM.BASE                       \n//S1   EXEC PGM=OEMPUT,REGION=0M,                 \n//  PARM=('-M&amp;QM -tm1 -Q&amp;Q -fileDD:MSGIN -P  ')   \n//SYSIN  DD *                                     \n/*                                                \n//STEPLIB  DD DISP=SHR,DSN=ZQS1.MP1B.LOAD         \n//         DD DISP=SHR,DSN=MQ940CD.SCSQLOAD       \n//         DD DISP=SHR,DSN=MQ940CD.SCSQAUTH       \n//         DD DISP=SHR,DSN=MQ940CD.SCSQANLE       \n//SYSPRINT DD SYSOUT=*                            \n//MSGIN    DD DISP=SHR,DSN=ZQS1.MQ.JCL(MSGS)      \n//                                                \n</code></pre> </li> <li> <p>Make sure that your queue manager and queue names are correct in lines 46 and 47.</p> </li> <li> <p>Once in OEMPUT, type \u2018submit\u2019 on the command line and hit enter to load persistent messages into the queue manager.</p> <p>I won\u2019t summarize the whole JCL, but pay attention to this particular line:  </p> <p><code>PARM=('-M&amp;QM -tm3 -Q&amp;Q -crlf -fileDD:MSGIN -P')</code></p> <p>Lets break it down:</p> Parameter Meaning '-M&amp;QM Queue manager name -tm3 Send messages for 3 minutes -Q&amp;Q The queue name -crlf Each line in the input message file is used in sequence as message data -fileDD:MSGIN Use the MSGIN file as input -P Use persistent messages </li> <li> <p>If you look at your MQ Explorer, you should now see that your queue is populated with lots of messages! </p> <p></p> </li> <li> <p>Back in ZQS1.MP1B.JCL, navigate to the SMFDUMP member. Once inside, modify the date to be accurate. If you are completing this lab on 2/24/2025 at SHARE, the date will be 2025055. Additionally adjust the START parameter to reflect the appropriate hh:MM. Your JCL should look something like:     <pre><code>//SYSIN  DD *                                          \nLSNAME(IFASMF.DEFAULT,OPTIONS(DUMP))                 \nOUTDD(DUMPOUT,TYPE(115,116),START(1230),END(2200))   \nDATE(2025055,2025055)     \n</code></pre></p> </li> </ol> <p>This is a date in the Julian format.</p> <ol> <li> <p>Enter \u2018submit\u2019 on the command line to execute SMFDUMP JCL. The SMFDUMP JCL starts with deleting old tasks, then outputs it in a specified location, in our case, ZQS1.QUEUE.MQSMF.SHRSTRMx.</p> </li> <li> <p>You can check that the SMFDUMP is processing by navigating to your job using SDSF. Access SDSF using =D from the ISPF menu.</p> </li> <li>Once in SDSF, select ST from the menu and hit \u2018enter\u2019</li> <li>Type in \u2018prefix ZQS1*\u2019. This will show you a list of all jobs submitted that start with ZQS1. Remember, we define our job names at the top left of each JCL file.  </li> <li>Here, you put a \u2018?\u2019 mark besides the jobname. Hit enter, then a screen with a SYSPRINT menu option should pop up. Next to SYSPRINT, put a \u2018s\u2019 and hit enter.</li> <li> <p>Enter \u2018bottom\u2019 on the command line and you should see a screen like below, indicating that records are being written. You can also confirm this by looking in the output for the SUMMARY ACTIVITY REPORT.</p> <p></p> </li> <li> <p>You will have to submit one final job MQSMFP in ZQS1.MP1B.JCL. This job will give us some formatted information about the SMF data. Type \u2018submit\u2019 and hit enter.</p> <pre><code>//****************************************************************\n//*                                                               \n//S1 EXEC PGM=MQSMF,REGION=0M                                     \n//STEPLIB  DD DISP=SHR,DSN=ZQS1.MP1B.LOAD                         \n//SMFIN    DD DISP=SHR,DSN=ZQS1.QUEUE.MQSMF.SHRSTRM6              \n//SYSIN    DD *                                                   \nDETAIL 5                                                         \nSMF_Interval_time 900                                            \n/*                                                                \n//SYSPRINT DD SYSOUT=*,DCB=(LRECL=200)                            \n//SYSOUT   DD SYSOUT=*,DCB=(RECFM=VB,LRECL=200,BLKSIZE=27998)     \n//SYSERR   DD SYSOUT=*                                            \n//ADAP     DD SYSOUT=*                                            \n//ADAPCSV  DD SYSOUT=*                                            \n//BUFF     DD SYSOUT=*,DCB=(LRECL=200)                            \n//BUFFIO   DD SYSOUT=*,DCB=(LRECL=200)                            \n//BUFFCSV  DD SYSOUT=*,DCB=(LRECL=200)                            \n//CF       DD SYSOUT=*                                            \n//CFCSV    DD SYSOUT=*                                            \n//CHINIT   DD SYSOUT=*                                            \n</code></pre> </li> <li> <p>Now, navigate to the SDSF output for the submitted job. We will be able to see the SMF output in useful categories that can also be exported as CSV files.</p> <p></p> </li> <li> <p>Navigate to the QSTATS statistics by putting a \u2018s\u2019 next to it and hitting enter. Once inside QSTATS, on the command line, enter:</p> <p>f MP1B.TESTER</p> </li> <li> <p>Voila, you should now see detailed information about the queue we set up, including the storage. </p> </li> </ol> <p></p>"},{"location":"queue-statistics/#iv-navigate-the-smf-data-output-to-find-performance-problems","title":"IV.    Navigate the SMF data output to find performance problems","text":""},{"location":"queue-statistics/#v-interpret-the-findings","title":"V. Interpret the findings","text":"<p>While we just went through QSTATS output, take some time to explore the other outputs we get with queue statistics. </p> <p>QALL <pre><code>Queue data summarised by queue                                                 \n          0 Open name                                  TEAM1.STREAM.BASE       \n          0 Queue type:  QLocal                        TEAM1.STREAM.BASE       \n          0 Page set ID                        4       TEAM1.STREAM.BASE       \n          0 Buffer pool                        3       TEAM1.STREAM.BASE       \n          0 Put count                      51057       TEAM1.STREAM.BASE       \n          0 Put avg elapsed time            1171 uS    TEAM1.STREAM.BASE       \n          0 Put avg CPU time                  47 uS    TEAM1.STREAM.BASE       \n          0 Put + put1 valid count         51057       TEAM1.STREAM.BASE       \n          0 Inq count                          1       TEAM1.STREAM.BASE       \n          0 Inq avg elapsed time              14 uS    TEAM1.STREAM.BASE       \n          0 Inq avg CPU time                  14 uS    TEAM1.STREAM.BASE       \n          0 Total queue elapsed time    59803352 uS    TEAM1.STREAM.BASE       \n          0 Total queue CPU used         2406244 uS    TEAM1.STREAM.BASE    \n</code></pre> QSTAT</p> <pre><code>Queue statistics                                                                \n\nMQS1,ZQS1,2025/02/24,09:00:06,VRM:940,                                          \n  From 2025/02/24,08:59:05 to 2025/02/24,09:00:05, duration   60 seconds.       \n\nMQS1,ZQS1,2025/02/24,09:00:06,VRM:940,                                          \nQueue Name.................................SYSTEM.PROTECTION.POLICY.QUEUE       \nDisposition................................Private                              \nPageset ID.................................Unallocated                          \nBufferpool ID..............................Unallocated                          \nCurrent Depth..............................0                                    \nOpen Output Count..........................0                                    \nOpen Input Count...........................0                                    \nQTIME Short................................0                                    \nQTIME Long.................................0                                    \nLast Put Time..............................                                     \nLast Get Time..............................                                     \nUncommitted Changes........................No                                   \n</code></pre> <p>QPUTSCSV provides all data relevant to putting messages onto the queue. <pre><code>Queue,Puts,Put1s,TotBytes,MaxMsgSz,MinMsgSz      \nTEAM1.STREAM.BASE,51057,0,40845600,800,800    \n</code></pre></p> <p>QGETSCSV provides all data relevant to getting messages from the queue. QSTATCSV provides all data related to handles, API calls other miscellaneous items.</p> <pre><code>z/OS,QM,Date,Time,Queue,Disp,PSID,BPID,QSG,CF,Dpth,OPPROC,IPPROC,QTIMES,QTIMEL,L\ncLow,IPProcHigh,IPProcLow,MQOPENs,MQCLOSEs,MQINQs,MQSETs,ExpiredMsgs,RecType    \nMQS1,ZQS1,2025/02/24,09:00:06,SYSTEM.PROTECTION.POLICY.QUEUE,Private,Unallocated\n0,Full                                                                          \nMQS1,ZQS1,2025/02/24,09:00:06,QCPY.INPUT,Private,Unallocated,Unallocated,,,0,0,0\nMQS1,ZQS1,2025/02/24,09:00:06,SYSTEM.JMS.ADMIN.QUEUE,Private,Unallocated,Unalloc\nMQS1,ZQS1,2025/02/24,09:00:06,TEAM1.STREAM.BASE,Private,4,3,,,66370,0,0,0,0,2025\n,0,0,0,0,0,Full                                                                 \nMQS1,ZQS1,2025/02/24,09:00:06,CICS01.INITQ,Private,Unallocated,Unallocated,,,0,0\nMQS1,ZQS1,2025/02/24,09:00:06,SYSTEM.JMS.ND.SUBSCRIBER.QUEUE,Private,Unallocated\n0,Full                                                                          \nMQS1,ZQS1,2025/02/24,09:00:06,SYSTEM.JMS.ND.CC.SUBSCRIBER.QUEUE,Private,Unalloca\n,0,0,Full                                                                       \nMQS1,ZQS1,2025/02/24,09:00:06,QCPY.SHARED.CONTROL,Shared,,,QSGA,TEST2       ,0,0\nMQS1,ZQS1,2025/02/24,09:00:06,QCPY.CONTROL,Private,Unallocated,Unallocated,,,0,0\nMQS1,ZQS1,2025/02/24,09:00:06,MSGSELECT.SHAREDQ,Shared,,,QSGA,TEST2       ,1,0,0\nMQS1,ZQS1,2025/02/24,09:00:06,ANSIBLE.DEMO.QUEUE,Private,Unallocated,Unallocated\nMQS1,ZQS1,2025/02/24,09:00:06,SYSTEM.DURABLE.SUBSCRIBER.QUEUE,Private,2,1,,,2,0,\n</code></pre> <p>QSUML data is a summary of the queue usage over time, for local queues.</p> <pre><code>Queue tree                                                                      \nDate,Time,Qmgr,Queue,Count,PS,BP,\"Put MB\",\"Get MB\",!,ValidPut,ValidGet,getpsn,Ma\n2025/02/24,11:00:00, G  ,TEAM1.STREAM.BASE                               ,   2, \n</code></pre> <p>QSUMS data is a summary of the queue usage over time, for shared queues.</p> <pre><code>Queue tree                                                                      \nDate,Time,Qmgr,Queue,Count,Structure,\"Put MB\",\"Get MB\",!,ValidPut,ValidGet,MaxQD\n</code></pre> <p>LAB FINISHED!</p>"},{"location":"streaming-queues/","title":"Creating Streaming queues w/ IBM MQ for z/OS","text":""},{"location":"streaming-queues/#audience-level","title":"Audience level","text":"<p>Some knowledge of MQ or z/OS </p>"},{"location":"streaming-queues/#skillset","title":"Skillset","text":"<p>MQ Administration</p>"},{"location":"streaming-queues/#background","title":"Background","text":"<p>This lab introduces using streaming queues to IBM MQ for z/OS. Streaming queues was released in the IBM MQ product in release version 9.2.3. The function enables you to send near-duplicate messages to a secondary queue, called the streaming queue. </p>"},{"location":"streaming-queues/#overview-of-the-exercise","title":"Overview of the exercise","text":""},{"location":"streaming-queues/#lab-begin","title":"Lab Begin","text":"<p>1)  If not already started, start the MQ Explorer by double clicking on the icon on the image.</p> <p></p> <p>2)  The application should start and show that it is at MQ Version 9.4</p> <p>3)  When the application has started, there may be a selection of queue managers available.  We are interested in connections to queue managers on our z/OS environment. If you do not see connections to a queue manager you have been working with, you can create a connection by right clicking \u2018Queue Managers\u2019 and selecting to \u2018Add Remote Queue Manager\u2026\u2019. You will then fill out all the necessary details for IP address and port number. \u2003 4)  Right click on ZQS1 and select connect.  Please note that the IP address may be different from what is shown here.  </p> <p>The description and command level show that this is a 9.4 queue manager on z/OS.  </p> <p></p> <p>5)  Expand the ZQS1 queue manager \u2013 click on the \u2018&gt;\u2019 beside the name to see the resources defined to this queue manager. </p> <p>6)  Click on the \u2018Queues\u2019 to see the queues that are currently defined.  Please note that the list may differ from what is shown here. For example, you may see the SYSTEM queues or temporary dynamic queues that are currently in use. </p> <p></p> <p>7)  Right click on the Queues under the ZQS1 queue manager and select New-&gt; Local Queue</p> <p></p> <p>8)  The new local queue dialog box should appear, and you will type in the queue name for the streaming target queue.  It should be TEAMXX.STREAM.COPY, replacing the TEAMXX with the TEAM number you have been assigned (it will be TEAM01-TEAM20). After entering the queue name, please click on the \u2018Next\u2019 button.</p> <p></p> <p>9)  On the Change Properties dialog box Please select the \u2018Extended\u2019 option and change the \u2018Sharability\u2019 to \u2018Sharable\u2019 and the \u2018Default input open option\u2019 to \u2018Input Shared as shown and click on the Finish Button.</p> <p></p> <p>10) The object should be successfully created, and the following dialog box should appear.  If you would like to check the \u2018do not show success messages in future\u2019 please feel free.  Please then click the OK button to clear the display. </p> <p>11) You will now define the base queue for streaming.  Right click on the queues tab again and select \u2018New\u2019 -&gt; \u2018Local Queue\u2019 to define the streaming base queue.  It\u2019s name will be TEAMXX.STREAM.BASE, replacing the TEAMXX with your team number (TEAM01 thru TEAM20) . </p> <p></p> <p>12) Click on the Next Button.  There are both Extended and Storage tab changes that will be made.  </p> <p>13) On the \u2018Extended\u2019 tab please change the Sharability and Default Open input option to \u2018Sharable\u2019 and \u2018Input Shared\u2019 as you did for the first queue. </p> <p>14) For those of you familiar with the Storage tab on this dialog box, there have been some changes.  The streaming queue name and quality of service are set here. </p> <p></p> <p>15) The queue name may be selected from the previously defined queues by using the \u2018Select\u2019  Button and choosing the name from the dialog box.  Please select the streaming target queue defined above, you may have to scroll down to find it, and click on \u2018OK\u2019.</p> <p>16) The queue name should now be populated in the dialog box.  Please then select \u2018Must duplicate\u2019 for the 'Streaming quality of service' and click on Finish.</p> <p></p> <p>17) To test, we are just going to put messages to the base queue.  From the queue list right click on the TEAMXX.STREAM.BASE queue (replacing TEAMXX with your team number) and select \u2018Put Test message\u2019.  In the dialog box, please enter a test message and click on the \u2018OK\u2019 button</p> <p></p> <p>18) Please put two more messages onto the queue, varying the contents a bit.  \u2018Test 1, Test2, Test 3\u2019 is just fine.  Click on the \u2018Close\u2019 button to return to the queue list.</p> <p>19) Click on the refresh key, in the upper right side of the queue list box to refresh the list of queues. </p> <p></p> <p>20) You should now see that both the base and copy queues have an equal number of messages.</p> <p>21) At this point you can browse the queues.  Note that the message contents are the same as are the Message IDs on both queues.  </p> <p>BASE:</p> <p></p> <p>COPY:</p> <p></p> <p>22) Testing an Exception \u2013 put inhibit the COPY queue.  From the list of queues, Right click on the TEAMXX.STREAM.COPY (USE YOUR TEAM NUMBER IN PLACE OF TEAMXX) queue and select Properties.  Select \u2018Inhibited\u2019 for put messages, and click on OK.</p> <p></p> <p>23) Attempt to put a message on the TEAMXX.BASE.QUEUE, replacing the TEAMXX with your team ID.  </p> <p></p> <p>24) You should receive a message that you cannot put a message to this queue.</p> <p></p> <p>25) Clicking on the \u2018Details\u2019 shows the reason:</p> <p> \u2003 26) Going back to the BASE queue, change the Streaming Quality of service from \u2018Must Duplicate\u2019 to \u2018Best effort\u2019 and click the OK button.  </p> <p> \u2003 27) Try to put another message to the BASE queue, like what is shown. </p> <p></p> <p>28) That should work, and the depths of the base and copy queues should now be different: </p> <p></p> <p>29) Congratulations!  You have now been able to create and use a streaming, private queue.  </p> <p>30) Now, we will use the queue-sharing group defined on the environment called QSGA to create a streaming, shared queue. </p> <p>31) On MQ explorer, under Queue-sharing groups, you should see QSGA defined. Click the drop-down to see a list of Shared Queues.  </p> <p>32) Unlike private queues, we will need to check which shared storage we should specify for our shared queues. Look under \u2018Coupling Facility Structures\u2019</p> <p>33) We will go ahead and remember TEST1 for our storage needs. Now, create a new shared queue.  </p> <p>34) We will start with defining our streaming queue like so.  </p> <p>35) Under the Storage settings, we must specify our Coupling facility structure name of choice. This is where we will put in TEST1. That is the only additional setting you will need to make for the streaming queue. Press finish.</p> <p>36) Now, we will define our base queue following the same process of creating a shared queue. Here, however, we will specify a streaming queue to point to like so. </p> <p>37) Now, we have defined two shared queues. You should see both in the shared queues list under QSGA.  </p> <p>38) Let\u2019s test them out! Right click the base queue and put a test message on the base queue. </p> <p>39) Once put, you should see the message duplicated on the streaming queue. </p> <p>40) When you navigate to the individual queue managers\u2019 queues, you should see both the shared queues and their messages available to both ZQS1 and ZQS2.</p> <p>LAB FINISHED!</p>"},{"location":"Blog/CF%20calls%20Generated%20by%20an%20MQGET%20Query/","title":"CF calls Generated by an MQGET Query","text":"<p>Lyn Elkins</p> <p>I am not an SQL expert, which everyone should know since I only recently discovered how to use VIEWs to make my tasks a bit easier. Any SQL I write and share should be reviewed with that in mind. These queries are also written based on the column names assigned by MQSMFCSV, which will be different is using another SMF interpreter. This is also considered open source and should be treated as such. It is presented as-is and there is no implied or stated support.</p> <p>Years ago, I began using the WQ records to provide summaries of queue level activity I briefly looked into the breakdown of Coupling facility calls that were being made. I was told by people with the correct accent that I really did not need to know about those fields, and for years I did not. However, as those people who assured me that I didn\u2019t need to know are gone, I ended up having to look into these fields and found that yes we do need examine those records in that level of detail because it led to an explanation for some behavior that has been observed several times now by customers that are heavy users of shared queues in various patterns. We also uncovered some gaps in the data collection that are being addressed by the development lab. This is complex performance problem determination, looking into things that are not observed everything is running well.</p> <p>If you have not read the articles on Task Records and how they are associated, Using Views, and symptoms that led to finding this issue (which has already been applied to some other customer data), I\u2019d recommend a quick overview of those first. The SQL development took several iterations to get meaningful information. We were fortunate to have customer data from days where the processing flowed normally and where they had problems. How often have I said, \u201cIt is hard to spot abnormal when you don\u2019t know what normal looks like\u201d? More than I can count certainly. In this case it was vital to finding the issues.</p> <p>First it was troubling that the overall volume of requests was not noticeably higher on some of the days where there were issues reported. That\u2019s unusual, as many of the problems we see are related to volume changes. When we started analyzing the traffic in the queues themselves, from the Task associated WQ records, we did find changes in volumes on individual queues. We also found that the overall mix of messages was skewed towards one queue on one structure during the problem periods. And we could see after the individual queue analysis that one queue had an extremely low valid get percentage when those periods began. What was also interesting, was that we could see the percentage of valid gets fluctuate on this queue even in \u2018good\u2019 periods at times almost dropping to the same level as when there were noted problems. We then started looking at the types of MQGETs being done against the shared queues in this QSG. That led to looking at the actual CF calls being made, and that made all the difference. The query used was just for the GET processing being done. When we tried to combine the PUT processing into the same query we had far too many columns to be contained in a normal spreadsheet. As it turned out, we did not need all of the calls to the CF that the MQGETs resolved into, so we did simplify the query to look for the \u2018READLIST\u2019 and \u2018GETMOVE\u2019 CF requests in our comparisons.</p> <p>This shorter version of the \u2018What calls are being made\u2019 query looks like what is shown below, and is included in this git repository as CFGETsBreakDown.txt.</p> <p>What we found was simple, the volume of requests skewed toward a particular type of processing when the issue began \u2013 that was to do gets by a message selector. This is more work for the queue manager as it has to read thru all the messages on the queue looking for the right match. Unlike private queues where there is a \u2018messages skipped\u2019 count there is no equivalent for shared queues. Also the use of message selectors only shows up in the WQ records when the queue is opened, and as these were very long running tasks at first glance it looked like request to the problem structure and queue were not using selectors. It was only when we looked at the actual CF calls that we could see that the processing for a particular queue on a specific structure that was different. Each MQGET was resolving into anywhere from a few (during non-problem periods) to several thousand \u2018READLIST\u2019 requests (during problem periods) to the CF structure. This was because as the queue got deeper more READLISTs were required to scroll thru all the messages. To make matters worse the applications issuing the MQGETs had a very short timeout (in seconds), but the messages themselves were not expiring for minutes. That meant that messages were building up on the queue that would never be retrieved while at the same time new messages were being put. The solution was to alter the processing to use an indexable field rather than a message selector, to set a message expiration closer to the value used for the MQGET, and to set the queue manager expiration interval to run more frequently on all queue managers in the QSG. The application changes took more time to implement, so to help prevent the issue from impacting other application and other processes that were part of this application, the queue was moved to its own CF structure so there would be less competition for resources and the queue manager expiry interval was set on all queue managers to help clean up the queue more frequently.</p> <p>It was also interesting to discover the issue was also present during period where there had not been a detected problem. There was a specific volume of failing MQGETs before the impact was detected by the application and users \u2013 when we reviewed some of the \u2018good\u2019 processing periods we also found failing MQGETs, and at times it seemed to border on the problem threshold. This was a classic problem waiting to happen.</p> <p>EXPORT TO \"E:\\customer\\Query_Results\\GETTasksByQ.csv\" OF DEL MODIFIED BY COLDEL, DECPT.</p> <p>SELECT</p> <p>CHAR(WTID.DATE) AS DATE1,</p> <p>WTID.TIME AS TIME1,</p> <p>WTID.LPAR AS LPAR,</p> <p>WTID.QMGR AS QMGR,</p> <p>WTID.WTAS_CORRELATOR AS CORRELID,</p> <p>WTID.CHANNEL_NAME AS CHL_NAME,</p> <p>WTID.Channel_Connection_Name AS CONNECTION_NAME,</p> <p>CHAR(WTAS.Commit_Count) AS COMMIT_COUNT,</p> <p>CHAR(WTAS.Commit_ET_us) AS COMMIT_ET,</p> <p>CHAR(WTAS.Commit_CT_us) AS COMMIT_CPU,</p> <p>CHAR(WTAS.Backout_Count) AS BACKOUT_COUNT,</p> <p>CHAR(WTAS.Backout_ET_us) AS BACKOUT_ET,</p> <p>CHAR(WTAS.Backout_CT_us) AS BACKOUT_CPU,</p> <p>CHAR(WTAS.START_TIME_DATE) AS TASK_START_DATE,</p> <p>CHAR(WTAS.START_TIME_TIME) AS TASK_START_TIME,</p> <p>CHAR(CF_STE_CALL_COUNT) AS SINGLE_ENTRY_CALL_COUNT,</p> <p>CHAR(CF_STM_CALL_COUNT) AS MULTIPLE_ENTRY_CALL_COUNT,</p> <p>CHAR(CF_STE_REDRIVE_COUNT) AS SINGLE_ENTRY_REDRIVES,</p> <p>CHAR(CF_STM_REDRIVE_COUNT) AS MULTIPLE_ENTRY_REDRIVES,</p> <p>CHAR(CF_STE_ELAPSED_US) AS SINGLE_ENTRY_ET,</p> <p>CHAR(CF_STM_ELAPSED_US) AS MULTIPLE_ENTRY_ET,</p> <p>WQ.Open_Name AS OPEN_NAME,</p> <p>WQ.Base_Name AS BASE_NAME,</p> <p>CHAR(WQ.GET_COUNT) AS GET_COUNT,</p> <p>CHAR(WQ.TOTAL_VALID_GETS) AS VALID_GET_COUNT,</p> <p>CHAR(WQ.GET_CT_US) AS GET_CPU,</p> <p>CHAR(WQ.GET_ET_US) AS GET_ET,</p> <p>CHAR(GET_DEST_ANY_COUNT) AS MQGET_ANY_COUNT,</p> <p>CHAR(GET_DEST_SPECIFIC_COUNT) AS MQGET_SPECIFIC_COUNT,</p> <p>CHAR(MAX_DEPTH) AS MAX_QUEUE_DEPTH,</p> <p>CHAR(SELECT_COUNT) AS SELECTOR_COUNT,</p> <p>CHAR(SELECT_MAX_LENGTH) AS SELECTOR_MAX_LEN,</p> <p>CHAR(CFCOUNT_GET_READLIST) AS CFREADLIST_COUNT,</p> <p>CHAR(CFSYNC_GET_READLIST) AS CFSYNC_READLIST_COUNT,</p> <p>CHAR(CFSYNC_GET_READLIST_ET) AS CFSYNC_READLIST_ET,</p> <p>CHAR(CFASYNC_GET_READLIST) AS CFASYNC_READLIST_COUNT,</p> <p>CHAR(CFASYNC_GET_READLIST_ET) AS CFASYNC_READLIST_ET,</p> <p>CHAR(CFCOUNT_GET_MOVE) AS CFGETMOVE_COUNT,</p> <p>CHAR(CFSYNC_GET_MOVE) AS CFSYNC_GETMOVE_COUNT,</p> <p>CHAR(CFSYNC_GET_MOVE) AS CFSYNC_GETMOVE_ET,</p> <p>CHAR(CFASYNC_GET_MOVE) AS CFASYNC_GETMOVE_COUNT,</p> <p>CHAR(CFASYNC_GET_MOVE) AS CFASYNC_GETMOVE_ET,</p> <p>'2' AS Row_ID</p> <p>FROM QSGQSGA_WTID AS WTID, QSGQSGA_WTAS AS WTAS, QSGQSGA_WQ WQ</p> <p>WHERE ( WTID.WTAS_Correlator = WTAS.Correl</p> <p>AND WQ.CORRELATION = WTAS_Correlator</p> <p>AND WTID.DATE = WTAS.DATE</p> <p>AND WQ.Date = WTAS.Date</p> <p>AND WQ.TIME = WTAS.TIME</p> <p>AND WTAS.TIME = WTID.TIME</p> <p>AND WTAS.QMgr = WTID.Qmgr</p> <p>AND WQ.QMGR = WTAS.QMgr</p> <p>AND WQ.CF_STRUCTURE &lt;&gt; ' '</p> <p>AND WQ.GET_COUNT &gt; 0 )</p> <p>UNION</p> <p>SELECT</p> <p>' Date ',</p> <p>' Time ',</p> <p>' LPAR ',</p> <p>' QMGR ',</p> <p>' Task Correlation ID ',</p> <p>' Channel Name ',</p> <p>' Channel Conection Name ',</p> <p>' Commit Count ',</p> <p>' Commit Elapsed Time ',</p> <p>' Commit CPU Time ',</p> <p>' Backout Count ',</p> <p>' Backout Elapsed Time ',</p> <p>' Backout t CPU Time ',</p> <p>' WTAS Start Date ',</p> <p>' WTAS Start Time ',</p> <p>' WTAS Single Entry CF Call Count ',</p> <p>' WTAS Multiple Entry CF Call Count ',</p> <p>' WTAS Single Entry Redrive Count ',</p> <p>' WTAS Multiplee Entry Redrive Count',</p> <p>' WTAS Single Entry Elapsed Time ',</p> <p>' WTAS Multiple Entry Elapsed Time ',</p> <p>' Queue Open Name ',</p> <p>' Queue Base Name ',</p> <p>' Get Count ',</p> <p>' Valid Get Count ',</p> <p>' Get CPU ',</p> <p>' Get Elapsed Time ',</p> <p>' Get Destructive Any ',</p> <p>' Get Destrcutive Specific ',</p> <p>' Maximum Queue Depth ',</p> <p>' Get Selector Count ',</p> <p>' Get Selector Max Length ',</p> <p>' Get Readlist Count ',</p> <p>' Get Sync Readlist Count ',</p> <p>' Get Sync Readlist Elapsed Time ',</p> <p>' Get ASync Readlist Count ',</p> <p>' Get ASync Readlist Elapsed Time ',</p> <p>' Get Getmove Count ',</p> <p>' Get Sync Getmove Count ',</p> <p>' Get Sync Getmove Elapsed Time ',</p> <p>' Get ASync Getmove Count ',</p> <p>' Get ASync Getmove Elapsed Time ',</p> <p>'1' AS Row_ID</p> <p>FROM SYSIBM.SYSDUMMY1 ORDER BY Row_ID</p> <p>;</p>"},{"location":"Blog/MultipleQSGAndViews/","title":"Multiple QSG And Views","text":"<p>Over the past year we have seen more customers with multiple queue sharing groups (QSG) in production. Managing the SMF data is more challenging when this is the case, especially when there are multiple QSGs that have the same structure names (we see this a lot) and the same shared queue names. This is because most of the MQ SMF data is completely queue manager centric. Except for the QCCT (channel initiator statistics) and the new QQST (queue statistics records) this important information does not appear in the others. So, when I am looking at the MQ reported Coupling Facility statistics for a structure named APP1, if that structure is in multiple QSGs, the simple query we have commonly used those results will be misleading. Misleading data leads to errors when trying to track down workload skewing and performance problems. And while those of us using this hope that the QSG name gets added to more records in the future, what do we at the WSC do to address this in the meantime?</p> <p>Up until 2022, I hand coded SQL. Then Dorothy coded up Python scripts to generate the 'usual suspects' queries for our health check work. To use those in situations where there are multiple queue sharing groups, we have then had to replicate and tailor those scripts to pull the data for the queue managers in individual QSGs. And we have been in situations where the customer's documentation about QSG members has not been kept up to date - new queue managers have been added, older queue managers have been retired, or (even worse!) been moved to a different QSG.</p> <p>Looking for simpler, more efficient, less subject to error methods, and because these multi-QSGs have become more common, we talked about reconfiguring the python scripts, etc. But as I was working with data over this past week, and Dorothy was fighting fires on another front, I decided to try using Db2 Views to separate the queue managers into the QSGs. I am not an SQL expert, and as we all know the IBM documentation can be quite effective when you know what you are looking for and painful to follow when you are searching for the answers to \u2018this is how you do this.\u2019 So, in case others are in this same position, I felt like it was time to write this up. The first thing to do was figure out which queue managers are in which queue sharing group. After loading the data processed by MQSMFCSV into Db2 we run this very simple SQL:</p> <p>The results from this query look something like this: What this shows is that there are two queue sharing groups represented each with 6 queue managers, and 7 standalone queue managers. Using this information, I created views over all the tables generated by MQSMFCSV. The pattern looks like this:</p> <p>[INSERT IMAGE]</p> <p>A view for the tables for the standalone queue managers was also created. What this allowed us to do was use the views to isolate the activity reported on the individual QSGs, making the queries simpler, more readable, and giving us more accurate data about the use of the CF, Db2 and SMDS. It also allowed us to review the queue and task activity by QSG, giving better insight into their use as well.</p> <p>To illustrate the simplification, the SQL WHERE clause we had to compose to retrieve the QEST data (the queue managers calls to a structure data) before creating the views looked like this: After creating the views, the same Where clause looks like this:</p> <p>[INSERT IMAGE]</p> <p>This does not seem like much, but using views has once again saved much needed time and prevented typos. And it means that we are looking at data from a single queue sharing group, that we do not have the possibility of comingling data from multiple QSGs and can evaluate more complex situations more accurately.</p> <p>Please note that all samples considered open source, feel free to use them but they are provided AS-IS. There is no guarantee of support. </p>"},{"location":"Blog/Symptoms%20that%20led%20to%20PD/","title":"Symptoms that led to problem determination","text":"<p>The following list of symptoms led us to an underlying problem of application requests flooding a coupling facility.</p> <p>1) MQ Statistics Symptom - The number of message managers gets was much lower than the number of data manager gets.</p> <pre><code>a. As we have noted, the message manager counts the number of MQGET requests that\ncome into the queue manager. The data manager reports the number of gets that are\nused to fulfill the request. When the Data Manager Gets exceeds the Message Manager\nGets, that is an early and high-level indicator of message selector use or of queue not\nbeing indexed properly. When the MM GET count is higher than the DM GET count that\nis an indication that GETs are being done against empty queues, so the request is not\npassed to the DM.\n\n    i. Note \u2013 for private queues, a count of skipped messages is kept in the WQ\n    records. There is no equivalent for shared queues.\nb. The Coupling Facility statistics records, QEST, showed a dramatic rise in the number of\nCoupling Facility redrive requests over the problem intervals.\n\n    i. True for both single entry and multiple entry requests, a relatively small number\n    of redrives is normal but when they exceed 20% during an interval, we try to\n    look into it.\n</code></pre> <p>2) Coupling Facility Symptoms</p> <pre><code>a. High CF CPU use \u2013 high CPU use can be from any number of things including workload\nspikes. However, it has been observed in several instances of scrolling thru queues at\nseveral customers. It requires examination of the Coupling Facility Activity Report, or its\nequivalent, based on the RMF data.\n\nb. Sub-channel waits reported \u2013 Again, not a specific symptom as there can be a few\nreasons for this to happen. One is an increase or spike in the number of requests to a\nspecific CF structure, causing contention on the subchannels that provide\ncommunication between the LPAR and CF. Other reasons we have seen this happen is\ndue to CF Links being taken offline, typically accidentally, and changing hardware\nconfigurations. Sub-channel waits are also reported as part of the Coupling Facility\nActivity Report\n</code></pre> <p>3) MQ Task Accounting Symptoms:</p> <pre><code>a. The percentage of valid gets was extremely low for at least one of the queues using the\nproblem structure. In this case the percentage of valid gets for all the queues on the\ntroubled structure was less than 1% in total and the GET count was very high for at least\none queue on that structure. That is cause for concern in most environments. Please\nsee \u2018comparing valid MQGETs\u2019 below for additional information.\n</code></pre> <p>4) Many MQGETs were for specific messages.</p> <pre><code>a. In the WQ records contains counts of each type of MQGET; destructive get for any\nmessage (FIFO), destructive get for a specific message, browse for any message (FIFO),\nand browse for a specific message.\n\nb. There was no index defined for the shared queue that exhibited the most specific\nmessage gets on the problem structure.\n\nc. As described above, these long running TXs do not have the \u2018select count\u2019 indicator set\nfor any interval that does not contain an open.\n</code></pre> <p>5) Comparing valid MQGETs with those that do not return data in the WQ records. As stated above, the number of valid gets was extremely low during the problem period. However, this symptom can be caused by application style choices and may not be easy to remedy. Some issues include:</p> <pre><code>a. Polling applications \u2013 often these are applications that do not use triggering or gets with\na wait interval, they repeat the MQGETs until a message is returned. This particular\npattern can be made worse by client applications that are poorly behaved (connect, do\none thing, disconnect, repeat). It is also the typical implementation pattern from some\napplication enablement tools, so programmers may not even be aware that this is what\nis happening.\n\nb. Gets with waits \u2013 MQ will signal all instances of an application in a wait state when a\nnew message arrives on a queue. This causes the MQGET request to be resent (as if\nnew). This can lead a high number of \u2018empty Gets\u2019 if the messages are being put at a\ncomparatively slow rate when compared to how quickly messages can be retrieved.\n\nc. Too many getting application instances \u2013 tuning the number of application instances to\npeak periods can lead to a lot of empty gets during the slower times.\n\nd. Related to c is the fact that we, the WSC, have seen customers adding instances of the\ngetting applications to address response slowdowns, making the problem much worse.\nWhen the slowdown is caused by too few instances of a processing application, adding\nmore helps. When it is caused by internal contention over a particular resource, more\ninstances make the problem worse.\n\ne. The last symptom we found was when we examined the calls to the CF to fulfill the\nrequests. I had never examined the data at this low a level before, having been told\noften enough that \u2018you should never need to.\u2019\n</code></pre> <p>The CF calls made depend on the type of MQGET being done. A simple FIFO MQGET is likely to only use a GETMOVE request to the CF \u2013 returning the next message available. An MQGET using a message property match is more complex. What we found was that the requests to get a message by a selector match issued one-to-many READLIST requests for each GETMOVE (the real get of the selected message). When things were running well, the ratio of READLISTs to GETMOVEs was no more than 3 to 1. Most often it was 1 to 1. During the problem period, the number of READLIST requests became exponentially higher than the GETMOVEs. The READLIST requests return a buffer of messages to the queue manager and will set a cursor on the queue to indicate the starting point for the next READLIST in case none of the messages in this group match the selection criteria. The number of messages that can be returned by the READLIST depends on the size of the buffer, the size of the messages, and any \u2018congestion\u2019 that may be taking place within the structure itself. With an increasing depth on the queue, the number of READLISTs increased because the GETs were requesting newer messages. The older messages had an expiration set and that was being honored, the problem would have been much worse if that were not the case. However, the message expiration was set much higher that the MQGET wait expiration, so there were messages on the queue that were never going to be matched and were being read multiple times.</p> <p>At the same time, there were applications continuing to put messages to the \u2018problem\u2019 queue, and other queues with active puts and gets on the same structure. None of the other queues had an extremely high volume at the time, but there was other activity on the CF structure.</p> <p>This is a good example of a problem waiting to happen \u2013 none of the processing was new or changed by the application or the queue manager. It reached a critical mass due to increased volume of certain types of requests, contention on the queue, and contention on the structure hosting that queue. In my next article I will publish the query that assisted us in finding the CF calls that were used. </p>"},{"location":"Blog/Task%20record%20information/","title":"Task Record Information","text":"<p>Task Records information</p> <p>A question we have already started receiving is whether we shall continue to need the Accounting class  3 records once customers have fully implemented the new queue statistics that became available in MQ  V9.3.3. The answer is simply yes, and we have a good example of why that detailed level of information  may be needed in this post. We have now found that there are occasions when we must look at the  details the requests made to Coupling Facility when using shared queues. This post is the beginning of  how we examined the queue manager conversion from an MQ API request to the CF requests. That  information is only found in the WQ records, part of the Task Accounting data.</p> <p>1) At the WSC we use MQSMFCSV to convert the MQ SMF records (all types and sub-types) to CSV  files and to generate the DDL for Db2. We use the DDL to build the tables within a Db2  database, then load the CSV files into the appropriate tables. The names of the fields used in  this document are those set by MQSMFCSV. If you are using a different SMF record parser or a  database other than Db2, this information will have to be translated into the formats used by  the different tools.</p> <p>2) A task to MQ is created for a connection to the QM from any type of workload (CICS, IMS, Db2,  RRS, TSO, Batch, CHIN, or client connections via the channel initiator). A task will always have a  task ID associated with it, it is a 33 byte character field that is used to correlate the WTID, WTAS  and WQ records associated with the task. The field has different names based on the record and  is called WTAS_CORRELATOR in the WTID, or task identification record, CORREL in the WTAS, or  Task Statistics record, and CORRELATION in the WQ or queue information record(s).</p> <p>3) The task records are always created at task end and may also be created at the SMF intervals for  tasks that span more than one SMF interval. These are known as Long Running Tasks (LRT). The  accounting SMF interval is controlled by the ACCTIME in the ZPRM member and defaults to -1 or  produce Accounting SMF at the same intervals as the Statistics.</p> <p>4) The WTID record contains information that remains consistent for the duration of the task, with  the exception of the \u2018Date and Time\u2019 fields. It also contains some potentially very useful  information like the connection name and application information.</p> <p>5) The WTAS record (tasks statistics) contains the date and time of the interval, the start date and  time for the task, and task specific data that we often use for performance issues \u2013 including  latching information and CF calls that are at the task level.</p> <p>6) The WQ records (task queue records) contain the interval data and time, and very detailed  information about the queue\u2019s disposition and use by this task. It includes specifics about the  MQAPI calls not found elsewhere and calls to the CF to satisfy the API request. The queue  statistics added with IBM MQ for z/OS 9.3.3 does not include this detailed breakdown of the API  requests.</p> <p>7) LRTs require special treatment, for example after the first set of records for the LRT, counts are  set to zero at SMF intervals.</p> <p>a) If you do not have the \u2018first instance\u2019 of the task in the data being examined, some  information may be hard to discern. For example, the use of selectors on MQGET processing.  The Selector count and length are set at MQOPEN time, and when an open is only done at the  start of a task subsequent records do not clearly indicate selector use. However, if all MQGET  requests show that they are for specific records and the queue is both shared and non-indexed  it is usually safe to assume that selectors are in use. </p> <p>b) Note that if an MQGET is done against a shared queue , has a 'typical' match option, and the  queue is not indexed properly; that MQGET will fail with a 2207 (CORREL_ID_ERROR).</p> <p>c) If the information is needed from start to finish for a particular task, the Accounting Class(3)  data should be started at queue manager start-up, or before the task is started and continue  until the task has ended, or until the next restart of the queue manager.</p> <p>8) To get the correct records to align for LRTs, several fields must be matched between the three  tables. They are listed here with the name of the table included as a qualifier: WTID.WTAS_CORRELATOR, WTAS.CORREL, WQ.CORRELATION WTID.DATE, WTAS.DATE, WQ.DATE WTID.TIME, WTAS.TIME, WQ.TIME WTID.LPAR, WTAS.LPAR, WQ.LPAR WTID.QMGR, WTAS.QMGR, WQ.QMGR</p> <p>9) The WHERE clause in queries to associate the correct WTID, WTAS and WQ records looks  something like this: WHERE (WTID.WTAS_CORRELATOR = WTAS.CORREL AND WTAS.CORREL = WQ.CORRELATION AND  WTID.DATE = WTAS.DATE AND WTAS.DATE = WQ.DATE AND WTID.TIME = WTAS.TIME AND  WTAS.TIME = WQ.TIME AND WTID.LPAR = WTAS.LPAR AND WTAS.LPAR = WQ.LPAR AND WTID.QMGR = WTAS.QMGR AND WTAS.QMGR = WQ.QMGR )</p> <p>10) The WHERE clause may be extended to return rows for just about anything, including a specific  date, queues that are on a specific structure (WQ.CF_STRUCTURE), a specific queue, or as in this  investigation looking for all queues hosted on a designated structure and where the get count  (WQ.GET_COUNT) is greater than zero.</p> <p>11) If your investigation includes data from more than one queue sharing group, defining views for  each QSG is necessary when the same structure names are used by more than one QSG. Please  see the previous article for directions on setting up those views.</p> <p>12) All MQ API requests are broken down into one or more calls to the coupling facility, depending  on the attributes of the request. This activity is reported in the WQ records, for queue-based activity, and the WTAS record for task related requests (commit, backout) and each record type  includes the range of CF requests that may be used to fulfill the MQ API request. We had to  focus on queue activity for this investigation.</p> <p>13) Don't feel daunted by the volume and complexity of the task related data, it can be  overwhelming when you first look at it. Thought is required to determine just how to  consolidate the data into useful information that can be used to make decisions about both  application and infrastructure patterns. It took several rounds of SQL attempts to get data in a  usable form with the information we were looking for, and we work with this kind of data  regularly.</p>"}]}